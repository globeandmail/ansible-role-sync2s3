#!/bin/bash
# {{ ansible_managed }}

SCRIPT_DIR=`dirname "$0"`
AWSCLI_EXE=`which aws`
WATCHPATH="{{ item.watch_path }}"
BUCKET_NAME="{{ item.bucket }}"
PIDFILE="$SCRIPT_DIR/sync2s3_{{item.name}}.pid"
export AWS_SHARED_CREDENTIALS_FILE="" # avoid using ~/.aws/credentials
export AWS_CONFIG_FILE="$SCRIPT_DIR/sync2s3_{{item.name}}.cfg"
POLLING_INTERVAL="15"
ERR_THRESHOLD="3"
HASH_IDX_FILE="$SCRIPT_DIR/processing/md5.hash.list_{{item.name}}"

# direct output to logfile
exec 2>&1
exec >> "$SCRIPT_DIR/sync2s3_{{item.name}}.log"

# preflight checks
if [ ! -x $AWSCLI_EXE ]; then echo "Could not find awscli!" ; exit 1 ; fi
if [ ! -d $WATCHPATH ]; then echo "Source dir (watchpath) does not exist"; exit 1 ; fi
if [ "$WATCHPATH" == "/" ]; then echo "WATCHPATH is root dir...?!" ; exit 1 ; fi
if [ "$BUCKET_NAME" == "" ]; then echo "No bucket name...?" ; exit 1 ; fi

# Is this task already running?
if [ -e $PIDFILE ]; then
  runningPid=$(cat $PIDFILE)
  if [ $runningPid -gt 0 ]; then
    if [ $(ps -fp $runningPid|grep `basename "$0"`|wc -l) -gt 0 ]; then
      exit 2
    fi
  fi
fi

# prep and create the pidfile
# TODO - wrong date is output (it retains the date from when the script started up)
trap "echo `date '+%Y-%m-%d %H:%M:%S'` - sync2s3_{{item.name}} exiting ; rm -f $PIDFILE ; exit" SIGINT SIGKILL SIGTERM SIGQUIT
echo $$ > "$PIDFILE"

# log startup
echo "`date '+%Y-%m-%d %H:%M:%S'` - sync2s3_{{item.name}} now watching $WATCHPATH for sync to s3://$BUCKET_NAME/{{item.name}}/..."

# remove hash file if it exists
[ -r "$HASH_IDX_FILE" ] && rm "$HASH_IDX_FILE"

# poll the src directory
failCount=0
prevHash=""
while [ $failCount -lt $ERR_THRESHOLD ]; do
    while read watchLine ; do
        if [ "$watchLine" == "" ]; then continue; fi

        # declare dir + file names
        dirName="`dirname "$watchLine"`"
        fName="`basename "$watchLine"`"

        if [ "$fName" == ".DS_Store" ]; then continue ; fi

        # check for file in md5 index
        if [ -r "$HASH_IDX_FILE" ]; then
          prevHash="$( egrep "$watchLine\$" "$HASH_IDX_FILE" | cut -d, -f1 )"
        else
          prevHash="none"
        fi

        ## calc the current hash based on last line of file (speeds things up a bit for large files)
        #thisHash=$(tail -n1 "$watchLine" | md5sum | awk '{print $1}')

        # calc the current hash based on file size
        thisHash=$(stat "$watchLine" | egrep -o "Size: [0-9]+" | md5sum | awk '{print $1}')

        # do they match?
        if [ "$thisHash" == "$prevHash" ]; then
          # empty files are quite possibly pending in 'finder'
          # note: 'find' should not be feeding any empty files to this while-loop, so if Size:0 happens, something ain't right.
          if [ "$(stat "$watchLine"|egrep -o "Size: [0-9]+")" == "Size: 0" ]; then
            echo "$watchLine appears to be stalled at 0b - waiting ..."
            continue;
          fi

          # Hashes match, move ahead
          [ -r "$HASH_IDX_FILE" ] && sed -i "\_$watchLine\$_d" "$HASH_IDX_FILE"
          echo "$thisHash,$watchLine" >> "$HASH_IDX_FILE"
        else
          # No hash match, loop again
          echo "$watchLine is new, or is still uploading..."
          [ -r "$HASH_IDX_FILE" ] && sed -i "\_$watchLine\$_d" "$HASH_IDX_FILE"
          echo "$thisHash,$watchLine" >> "$HASH_IDX_FILE"
          continue
        fi

        # determine the s3 obj key
        prefix=`echo $dirName | sed "s|^$WATCHPATH/\?||"`
        if [ "$prefix" == "" ]; then
            targetPath="{{item.name}}/$fName"
        else
            targetPath="{{item.name}}/$prefix/$fName"
        fi

        # skip files that don't actually exist (or we can't read)
        if [ ! -r "$dirName/$fName" ]; then continue; fi

        # log each transfer
        echo -n "`date '+%Y-%m-%d %H:%M:%S'` : '$dirName/$fName' to 's3://$BUCKET_NAME/$targetPath' ... "

        # explicitly set mime-type for .vtt files
        if [[ "$fName" = *".vtt" ]]; then
            $AWSCLI_EXE s3 cp "$dirName/$fName" "s3://$BUCKET_NAME/$targetPath" --no-guess-mime-type --content-type 'text/vtt' --only-show-errors
        else
            $AWSCLI_EXE s3 cp "$dirName/$fName" "s3://$BUCKET_NAME/$targetPath" --only-show-errors
        fi

        # remove file to indicate successful upload, but leave the file alone if cp fails
        if [ $? == 0 ]; then
            echo "OK"
            [ -r "$dirName/$fName" ] && rm "$dirName/$fName"
        else
            echo "FAIL"
            failCount=$(expr $failCount + 1)
        fi

    done <<< "$( find "$WATCHPATH" -type f ! -empty ! -name '.DS_Store' )"

    # clean up empty subdirs (but maybe we want to preserve the dir structure; commented out!)
    # find "$WATCHPATH/"* -maxdepth 0 -type d | while read tDir ; do
    #     if [ "$(find "$tDir" -type f | wc -l)" == "0" ]; then
    #         echo "Removing $tDir ..."
    #         rm -rf "$tDir"
    #     fi
    # done
    sleep $POLLING_INTERVAL
done

echo "Aborting -- Error threshold exceeded!"
rm -f "$PIDFILE"